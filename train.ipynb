{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mangalm96/explainablenlp-info259/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyEizW0NPJVc",
        "outputId": "66514081-a3e7-4017-b027-164d8db095a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ],
      "source": [
        "!python -V"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-_EYEUkPMpX",
        "outputId": "d749bea3-4681-4a1f-8d1f-5d884cdd491c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data = open(\"/content/gdrive/MyDrive/NLP Stance Detection/Code_Kaushal/data/wtwt/data.json\")\n",
        "data_list = json.load(data)"
      ],
      "metadata": {
        "id": "9C83n8RAbarx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stance_dict = dict()\n",
        "for record in data_list:\n",
        "  stance = record[\"stance\"]\n",
        "  stance_dict[stance] = stance_dict.get(stance, 0) + 1"
      ],
      "metadata": {
        "id": "aE6DodHacD7M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stance_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRxk-BMCcUk-",
        "outputId": "afb5ca41-95b5-41fd-b71b-22c045a3c84a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'comment': 18087, 'refute': 3791, 'support': 5951, 'unrelated': 16169}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CVS_AET_dict=dict()\n",
        "CI_ESRX_dict = dict()\n",
        "ANTM_CI_dict = dict()\n",
        "AET_HUM_dict = dict()\n",
        "DIS_FOXA_dict = dict()\n",
        "for record in data_list:\n",
        "  stance = record[\"stance\"]\n",
        "  merger = record[\"merger\"]\n",
        "  if merger == 'CVS_AET':\n",
        "    CVS_AET_dict[stance] = CVS_AET_dict.get(stance, 0) + 1\n",
        "  elif merger == 'CI_ESRX':\n",
        "    CI_ESRX_dict[stance] = CI_ESRX_dict.get(stance, 0) + 1\n",
        "  elif merger == 'ANTM_CI':\n",
        "    ANTM_CI_dict[stance] = ANTM_CI_dict.get(stance, 0) + 1\n",
        "  elif merger == 'AET_HUM':\n",
        "    AET_HUM_dict[stance] = AET_HUM_dict.get(stance, 0) + 1\n",
        "  else:\n",
        "    DIS_FOXA_dict[stance] = DIS_FOXA_dict.get(stance, 0) + 1\n"
      ],
      "metadata": {
        "id": "Qj3FVrLDlzMc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(CVS_AET_dict)\n",
        "print(CI_ESRX_dict)\n",
        "print(ANTM_CI_dict)\n",
        "print(AET_HUM_dict)\n",
        "print(DIS_FOXA_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY4_HsKKoKk7",
        "outputId": "40613550-b091-45ad-b3eb-5cca25610620"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'support': 2225, 'comment': 4899, 'unrelated': 2636, 'refute': 447}\n",
            "{'support': 688, 'unrelated': 461, 'comment': 852, 'refute': 225}\n",
            "{'support': 881, 'unrelated': 4444, 'comment': 2811, 'refute': 1791}\n",
            "{'support': 937, 'comment': 2538, 'unrelated': 2523, 'refute': 998}\n",
            "{'comment': 6987, 'unrelated': 6105, 'support': 1220, 'refute': 330}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==3.5.0\n",
        "!pip install wandb==0.9.4\n",
        "!pip install scikit-learn==0.23.1\n",
        "!pip install scipy==1.5.0\n",
        "!pip install ekphrasis==0.5.1\n",
        "!pip install emoji"
      ],
      "metadata": {
        "id": "x6tCpuzVPWLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1820e066-b71e-4af9-8927-ce66ab562c48"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==3.5.0\n",
            "  Downloading transformers-3.5.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (3.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (21.3)\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 38.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (3.17.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (4.63.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 47.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.3\n",
            "  Downloading tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 27.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.5.0) (3.0.7)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.5.0) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.0) (1.1.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.49 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n",
            "Collecting wandb==0.9.4\n",
            "  Downloading wandb-0.9.4-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=0.4.0\n",
            "  Downloading sentry_sdk-1.5.8-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 51.1 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.9.4) (7.352.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb==0.9.4) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.9.4) (1.15.0)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting watchdog>=0.8.3\n",
            "  Downloading watchdog-2.1.7-py3-none-manylinux2014_x86_64.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting gql==0.2.0\n",
            "  Downloading gql-0.2.0.tar.gz (18 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.9.4) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from wandb==0.9.4) (3.13)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.9.4) (2.23.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.9.4) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "  Downloading graphql-core-1.1.tar.gz (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from gql==0.2.0->wandb==0.9.4) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb==0.9.4) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->wandb==0.9.4) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->wandb==0.9.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->wandb==0.9.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->wandb==0.9.4) (2021.10.8)\n",
            "Building wheels for collected packages: gql, graphql-core, subprocess32\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-py3-none-any.whl size=7638 sha256=4039d49c16dbc54c4c7f3a9847ca5b8750aafb7db422f7498b7a7c15a56b1889\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/9a/56/5456fd32264a8fc53eefcb2f74e24e99a7ef4eb40a9af5c905\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-py3-none-any.whl size=104649 sha256=2c6d6efe8233eec46a9f516ccde5f13abd6c7bb84579a890b8dea73df43787d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/fd/8c/a20dd591c1a554070cc33fb58042867e6ac1c85395abe2e57a\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=4b0ff352e5d3895a90dbe20200f22e6febbe65b0d6d286f1cd1e423c9a8a3c3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "Successfully built gql graphql-core subprocess32\n",
            "Installing collected packages: smmap, graphql-core, gitdb, watchdog, subprocess32, shortuuid, sentry-sdk, gql, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.27 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 gql-0.2.0 graphql-core-1.1 sentry-sdk-1.5.8 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 wandb-0.9.4 watchdog-2.1.7\n",
            "Collecting scikit-learn==0.23.1\n",
            "  Downloading scikit_learn-0.23.1-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1) (1.1.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.1 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.23.1\n",
            "Collecting scipy==1.5.0\n",
            "  Downloading scipy-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from scipy==1.5.0) (1.21.5)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.5.0\n",
            "Collecting ekphrasis==0.5.1\n",
            "  Downloading ekphrasis-0.5.1.tar.gz (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1) (4.63.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting ujson\n",
            "  Downloading ujson-5.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1) (3.2.5)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1) (1.21.5)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis==0.5.1) (0.2.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis==0.5.1) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis==0.5.1) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis==0.5.1) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis==0.5.1) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis==0.5.1) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis==0.5.1) (1.15.0)\n",
            "Building wheels for collected packages: ekphrasis\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-py3-none-any.whl size=82842 sha256=6b36c7e1e2a64d6db5a881a7b885ef53635ee11449f48195020da36b91c9abb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/ec/0d/12659e32faf780546945d0120f2c8410eb3efb7426731da88f\n",
            "Successfully built ekphrasis\n",
            "Installing collected packages: ujson, ftfy, colorama, ekphrasis\n",
            "Successfully installed colorama-0.4.4 ekphrasis-0.5.1 ftfy-6.1.1 ujson-5.2.0\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 5.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=a62419e67bff28ae8f271a8c0062b959f5808854e37c38680cc40d65aab675fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31CqjrIYbGEt",
        "outputId": "b939c771-49fe-46bd-8a7f-a282623b69ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 6.9 kB/s \n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automated run"
      ],
      "metadata": {
        "id": "ZhKFpeT30rW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python gdrive/My\\ Drive/NLP\\ Project/bias-stance/bertloader.py --target_merger=CI_ESRX --bert_type=vinai/bertweet-base --dataset_name=wtwt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRjwnQYXneXq",
        "outputId": "d5a43347-a3ed-49be-d33c-ca0f397e0178"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++++++++++++++++++++++\n",
            "Target = True\n",
            "++++++++++++++++++++++\n",
            "{'comment': 0, 'unrelated': 1, 'support': 2, 'refute': 3} || {0: 'comment', 1: 'unrelated', 2: 'support', 3: 'refute'}\n",
            "Length of train, valid, test: 27130 0 2226\n",
            "Length of train, eval_set: 27130 2226\n",
            "Before shuffling train[0] =  944165633889095681 | After shuffling train[0] =  831878268378816513\n",
            "/content/gdrive/.shortcut-targets-by-id/1cqRKadLOmUTbjUjku8ixQublzk8n8F2_/NLP Project/bias-stance/saves/wtwt_CI_ESRX_target\n",
            "1. Type of train dataset\n",
            "27130\n",
            "{'tweet_id': '831878268378816513', 'merger': 'ANTM_CI', 'stance': 'support', 'text': ['Anthem', 'Sues', 'Cigna', 'To', 'Prevent', 'Termination', 'Of', 'Their', '$', '<number>', 'Billion', 'Dollar', 'Merger', 'Deal', 'anthem', 'cigna', 'merger', 'reuters'], 'target': 'Anthem Cigna'}\n",
            "Downloading: 100% 558/558 [00:00<00:00, 437kB/s]\n",
            "Downloading: 100% 843k/843k [00:00<00:00, 926kB/s]\n",
            "Downloading: 100% 1.08M/1.08M [00:01<00:00, 992kB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
            "Loaded Bert Tokenizer\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Train_dataset: num_batches= 1696  | num_data= 27130\n",
            "97\n",
            "Eval_dataset: num_batches= 140  | num_data= 2226\n",
            "97\n",
            "Train_dataset Size = 1696 Eval_dataset Size = 140\n",
            "1696\n",
            "(tensor([[    0, 22250,     3, 12691,     4,  2993,  2336,   144, 22250,   144,\n",
            "             3,   144,     3,   144,  7444, 22250, 13606,  9492,  7761,     2,\n",
            "             2, 17336, 42182,   880,     2,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1],\n",
            "        [    0,     3,  9568,  1163,  5061,   633,    52,  4978,  4749,     9,\n",
            "         17336,  2993,     3,  7051,  4521,     2,     2, 17336, 42182,   880,\n",
            "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1],\n",
            "        [    0, 12421, 55119,   708,  2047,  8536, 14659,     3,     7,     3,\n",
            "             7,     3,     2,     2,   327,  1828,   880,  7110,  1706,     2,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1],\n",
            "        [    0,     3,  5763, 57274,   348,   619,   490, 17336, 33494, 10523,\n",
            "            22,   104,     3, 22507,   152,    31,    11,    26, 12446,  1278,\n",
            "            19,  2351,     4,     2,     2,   327,  1828,   880,  7110,  1706,\n",
            "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1],\n",
            "        [    0,   125,     3,    55, 17336,    32, 29541,     7,    11,  1330,\n",
            "            34,     3,    15,     3,  6797,     9, 17336,    34,   927,  9984,\n",
            "          7789, 23538,  4918,   144,   144,     2,     2, 17336, 42182,   880,\n",
            "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1],\n",
            "        [    0,   196,    64,     8,   307,     8,   152,    46,    26,  2784,\n",
            "            21,    26,   695,     7, 29769, 10473,     3,     2,     2, 38424,\n",
            "         24517,  1960,   327,  1828,   880,     2,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1],\n",
            "        [    0,   122,   144, 64001,    16,     3,    24,    66,     3,   144,\n",
            "         54266,   151,    32,    16,     6,   960,    15,     3,   144, 29769,\n",
            "             3,   144,     3,     3,     2,     2, 38424, 24517,  1960,   327,\n",
            "          1828,   880,     2,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1],\n",
            "        [    0,   165,   158,     6,   127, 17336,    13, 29769,     3,   810,\n",
            "           271,    19,     3,    21,     2,     2, 38424, 24517,  1960,   327,\n",
            "          1828,   880,     2,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1],\n",
            "        [    0,   295, 12873,   317,    22,     8,    43,     9,  2650,    18,\n",
            "             7,  2650,    18,    12,   295,  6347,   317,    22,     8,    43,\n",
            "             9,   560,    18,     7,   560,    18,    12,   295, 22716,   317,\n",
            "            22,     8,    43,     9, 29541,    18,     7, 29541,    18,    12,\n",
            "           295, 16203,   317,    22,  1418,   361, 64001,     2,     2, 17336,\n",
            "         42182,   880,     2],\n",
            "        [    0,  2780,  2116,    70, 12446,  4127,  4783,    19, 17336,     4,\n",
            "         17336,    34,     3, 23536,  4647,     9,   297,   217,   223,     2,\n",
            "             2, 17336, 42182,   880,     2,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1]], device='cuda:0'), tensor([1, 3, 1, 0, 0, 2, 0, 1, 1, 0], device='cuda:0'), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0]], device='cuda:0'), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0]], device='cuda:0'))\n",
            "Sun Apr 10 00:05:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    59W / 149W |    347MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gdrive/My\\ Drive/NLP\\ Project/bias-stance/train.py --dataset_name=wtwt --target_merger=CI_ESRX --test_mode=True --bert_type=vinai/bertweet-base"
      ],
      "metadata": {
        "id": "F9Wk-jYCPRlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python gdrive/My\\ Drive/NLP\\ Project/bias-stance/train.py --dataset_name=wtwt --target_merger=CI_ESRX --test_mode=True --bert_type=vinai/bertweet-base --notarget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyGwQo0v61HO",
        "outputId": "b136a55f-ac45-4b66-a87b-8452fb0b16d0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++++++++++++++++++++++\n",
            "Target = False\n",
            "++++++++++++++++++++++\n",
            "Traceback (most recent call last):\n",
            "  File \"gdrive/My Drive/NLP Project/bias-stance/train.py\", line 8, in <module>\n",
            "  File \"/content/gdrive/.shortcut-targets-by-id/1cqRKadLOmUTbjUjku8ixQublzk8n8F2_/NLP Project/bias-stance/bertloader.py\", line 5, in <module>\n",
            "    from transformers import AutoTokenizer\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/__init__.py\", line 22, in <module>\n",
            "    from .integrations import (  # isort:skip\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/integrations.py\", line 81, in <module>\n",
            "    from .file_utils import is_torch_tpu_available  # noqa: E402\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 59, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 51, in <module>\n",
            "    from ._api.v2 import compat\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/__init__.py\", line 37, in <module>\n",
            "    from . import v1\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\", line 30, in <module>\n",
            "    from . import compat\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\", line 37, in <module>\n",
            "    from . import v1\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\", line 47, in <module>\n",
            "    from tensorflow._api.v2.compat.v1 import lite\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\", line 9, in <module>\n",
            "    from . import experimental\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\", line 8, in <module>\n",
            "    from . import authoring\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\", line 8, in <module>\n",
            "    from tensorflow.lite.python.authoring.authoring import compatible\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/authoring/authoring.py\", line 43, in <module>\n",
            "    from tensorflow.lite.python import convert\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py\", line 29, in <module>\n",
            "    from tensorflow.lite.python import util\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/util.py\", line 51, in <module>\n",
            "    from jax import xla_computation as _xla_computation\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/jax/__init__.py\", line 135, in <module>\n",
            "    from jax import nn as nn\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/jax/nn/__init__.py\", line 21, in <module>\n",
            "    from jax._src.nn.functions import (\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/jax/_src/nn/functions.py\", line 29, in <module>\n",
            "    from jax.scipy.special import expit\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/jax/scipy/__init__.py\", line 19, in <module>\n",
            "    from jax.scipy import signal as signal\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/jax/scipy/signal.py\", line 17, in <module>\n",
            "    from jax._src.scipy.signal import (\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/jax/_src/scipy/signal.py\", line 15, in <module>\n",
            "    import scipy.signal as osp_signal\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/signal/__init__.py\", line 307, in <module>\n",
            "    from ._peak_finding import *\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/signal/_peak_finding.py\", line 8, in <module>\n",
            "    from scipy.stats import scoreatpercentile\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/stats/__init__.py\", line 388, in <module>\n",
            "    from .stats import *\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py\", line 180, in <module>\n",
            "    from . import distributions\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/stats/distributions.py\", line 12, in <module>\n",
            "    from . import _discrete_distns\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/stats/_discrete_distns.py\", line 703, in <module>\n",
            "    planck = planck_gen(a=0, name='planck', longname='A discrete exponential ')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/stats/_distn_infrastructure.py\", line 2880, in __init__\n",
            "    locscale_out='loc, 1')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/stats/_distn_infrastructure.py\", line 701, in _construct_argparser\n",
            "    exec(parse_arg_template % dct, ns)\n",
            "  File \"<string>\", line 2, in <module>\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "byrwwrQA2vaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## individual script run"
      ],
      "metadata": {
        "id": "zgb8EqJGy0AY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "6Jlw1zW0uHUD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target_merger=['CVS_AET', 'ANTM_CI', 'AET_HUM', 'CI_ESRX']"
      ],
      "metadata": {
        "id": "-fDB-gVTqy8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "!python gdrive/My\\ Drive/NLP\\ Stance\\ Detection/Code_Kaushal/train.py --dataset_name=wtwt --target_merger='CVS_AET' --test_mode=False --bert_type=vinai/bertweet-base\n",
        "print('Total time taken (mins): ', int((time.time()-start_time)/60))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdqK-rby0tkN",
        "outputId": "64a56064-2308-4df8-d2da-033f55c0c4ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++++++++++++++++++++++\n",
            "Target = True\n",
            "++++++++++++++++++++++\n",
            "{'comment': 0, 'unrelated': 1, 'support': 2, 'refute': 3} || {0: 'comment', 1: 'unrelated', 2: 'support', 3: 'refute'}\n",
            "Length of train, valid, test: 19149 0 10207\n",
            "Length of train, eval_set: 15316 3833\n",
            "Before shuffling train[0] =  971761970117357568 | After shuffling train[0] =  547555882322919424\n",
            "/content/gdrive/My Drive/NLP Stance Detection/Code_Kaushal/saves/wtwt_CVS_AET_target\n",
            "1. Type of train dataset\n",
            "Length of train: 15316\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
            "Loaded Bert Tokenizer\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Train_dataset: num_batches= 958  | num_data= 15316\n",
            "88\n",
            "Eval_dataset: num_batches= 240  | num_data= 3833\n",
            "88\n",
            "Dataset created\n",
            "Sun Apr 10 04:26:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    58W / 149W |    331MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Model created\n",
            "Sun Apr 10 04:26:41 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    58W / 149W |    331MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Embedding Shape: torch.Size([64004, 768])\n",
            "134905348\n",
            "Detected 1 GPUs!\n",
            "\n",
            "\n",
            "========= Beginning 1 epoch ==========\n",
            "Train loss at 0: 21.77969741821289\n",
            "Train loss at 100: 21.462890625\n",
            "Train loss at 200: 19.169490814208984\n",
            "Train loss at 300: 22.715829849243164\n",
            "Train loss at 400: 23.292455673217773\n",
            "Train loss at 500: 20.52456283569336\n",
            "Train loss at 600: 19.461069107055664\n",
            "Train loss at 700: 22.586360931396484\n",
            "Train loss at 800: 17.20859718322754\n",
            "Train loss at 900: 20.897432327270508\n",
            "EVALUATING:\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Valid_loss 20.899546881516773\n",
            "[[   0 1256    0    0]\n",
            " [   0 1436    0    0]\n",
            " [   0  333    0    0]\n",
            " [   0  808    0    0]]\n",
            "comment F1-score: 0.0\n",
            "unrelated F1-score: 0.5450749667868666\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.3746412731541873\n",
            "F1-Weighted 0.2042075795215081\n",
            "F1-Avg 0.13626874169671666\n",
            "[0/5]     train_loss: 20.74319 valid_loss: 20.89955\n",
            "\n",
            "\n",
            "========= Beginning 2 epoch ==========\n",
            "Train loss at 0: 20.87512969970703\n",
            "Train loss at 100: 21.54987144470215\n",
            "Train loss at 200: 18.806442260742188\n",
            "Train loss at 300: 22.960424423217773\n",
            "Train loss at 400: 22.701114654541016\n",
            "Train loss at 500: 20.871295928955078\n",
            "Train loss at 600: 20.013805389404297\n",
            "Train loss at 700: 22.59218406677246\n",
            "Train loss at 800: 17.4919490814209\n",
            "Train loss at 900: 20.4066104888916\n",
            "EVALUATING:\n",
            "Valid_loss 20.824618951479593\n",
            "[[   0 1256    0    0]\n",
            " [   0 1436    0    0]\n",
            " [   0  333    0    0]\n",
            " [   0  808    0    0]]\n",
            "comment F1-score: 0.0\n",
            "unrelated F1-score: 0.5450749667868666\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.3746412731541873\n",
            "F1-Weighted 0.2042075795215081\n",
            "F1-Avg 0.13626874169671666\n",
            "[1/5]     train_loss: 20.70320 valid_loss: 20.82462\n",
            "\n",
            "\n",
            "========= Beginning 3 epoch ==========\n",
            "Train loss at 0: 20.285097122192383\n",
            "Train loss at 100: 21.400556564331055\n",
            "Train loss at 200: 18.592275619506836\n",
            "Train loss at 300: 23.028799057006836\n",
            "Train loss at 400: 22.729516983032227\n",
            "Train loss at 500: 20.9951171875\n",
            "Train loss at 600: 20.219966888427734\n",
            "Train loss at 700: 22.276260375976562\n",
            "Train loss at 800: 17.3631649017334\n",
            "Train loss at 900: 20.179792404174805\n",
            "EVALUATING:\n",
            "Valid_loss 20.77069340546926\n",
            "[[   0 1256    0    0]\n",
            " [   0 1436    0    0]\n",
            " [   0  333    0    0]\n",
            " [   0  808    0    0]]\n",
            "comment F1-score: 0.0\n",
            "unrelated F1-score: 0.5450749667868666\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.3746412731541873\n",
            "F1-Weighted 0.2042075795215081\n",
            "F1-Avg 0.13626874169671666\n",
            "[2/5]     train_loss: 20.67378 valid_loss: 20.77069\n",
            "\n",
            "\n",
            "========= Beginning 4 epoch ==========\n",
            "Train loss at 0: 20.381479263305664\n",
            "Train loss at 100: 21.02817153930664\n",
            "Train loss at 200: 18.394149780273438\n",
            "Train loss at 300: 22.882322311401367\n",
            "Train loss at 400: 23.05649185180664\n",
            "Train loss at 500: 20.721881866455078\n",
            "Train loss at 600: 20.120098114013672\n",
            "Train loss at 700: 22.382587432861328\n",
            "Train loss at 800: 17.13214683532715\n",
            "Train loss at 900: 20.59381866455078\n",
            "EVALUATING:\n",
            "Valid_loss 20.75338056484858\n",
            "[[   0 1256    0    0]\n",
            " [   0 1436    0    0]\n",
            " [   0  333    0    0]\n",
            " [   0  808    0    0]]\n",
            "comment F1-score: 0.0\n",
            "unrelated F1-score: 0.5450749667868666\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.3746412731541873\n",
            "F1-Weighted 0.2042075795215081\n",
            "F1-Avg 0.13626874169671666\n",
            "[3/5]     train_loss: 20.66905 valid_loss: 20.75338\n",
            "\n",
            "\n",
            "========= Beginning 5 epoch ==========\n",
            "Train loss at 0: 20.0744571685791\n",
            "Train loss at 100: 21.29922103881836\n",
            "Train loss at 200: 18.418468475341797\n",
            "Train loss at 300: 23.153491973876953\n",
            "Train loss at 400: 22.800443649291992\n",
            "Train loss at 500: 20.923276901245117\n",
            "Train loss at 600: 19.932697296142578\n",
            "Train loss at 700: 22.460180282592773\n",
            "Train loss at 800: 17.271390914916992\n",
            "Train loss at 900: 20.52741241455078\n",
            "EVALUATING:\n",
            "Valid_loss 20.745876944065095\n",
            "[[   0 1256    0    0]\n",
            " [   0 1436    0    0]\n",
            " [   0  333    0    0]\n",
            " [   0  808    0    0]]\n",
            "comment F1-score: 0.0\n",
            "unrelated F1-score: 0.5450749667868666\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.3746412731541873\n",
            "F1-Weighted 0.2042075795215081\n",
            "F1-Avg 0.13626874169671666\n",
            "[4/5]     train_loss: 20.65552 valid_loss: 20.74588\n",
            "Total time taken (mins):  27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "!python gdrive/My\\ Drive/NLP\\ Stance\\ Detection/Code_Kaushal/train.py --dataset_name=wtwt --target_merger='ANTM_CI' --test_mode=False --bert_type=vinai/bertweet-base\n",
        "print('Total time taken (mins): ', int((time.time()-start_time)/60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPAovL_-tFS2",
        "outputId": "f824052d-6de7-41b2-9ec8-723eddaa589e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++++++++++++++++++++++\n",
            "Target = True\n",
            "++++++++++++++++++++++\n",
            "{'comment': 0, 'unrelated': 1, 'support': 2, 'refute': 3} || {0: 'comment', 1: 'unrelated', 2: 'support', 3: 'refute'}\n",
            "Length of train, valid, test: 19429 0 9927\n",
            "Length of train, eval_set: 15540 3889\n",
            "Before shuffling train[0] =  971761970117357568 | After shuffling train[0] =  976194260625252352\n",
            "/content/gdrive/My Drive/NLP Stance Detection/Code_Kaushal/saves/wtwt_ANTM_CI_target\n",
            "1. Type of train dataset\n",
            "Length of train: 15540\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
            "Loaded Bert Tokenizer\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Train_dataset: num_batches= 972  | num_data= 15540\n",
            "97\n",
            "Eval_dataset: num_batches= 244  | num_data= 3889\n",
            "97\n",
            "Dataset created\n",
            "Sun Apr 10 04:54:12 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P0    59W / 149W |    335MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Model created\n",
            "Sun Apr 10 04:54:15 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0    60W / 149W |    335MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Embedding Shape: torch.Size([64004, 768])\n",
            "134905348\n",
            "Detected 1 GPUs!\n",
            "\n",
            "\n",
            "========= Beginning 1 epoch ==========\n",
            "Train loss at 0: 22.304283142089844\n",
            "Train loss at 100: 14.465860366821289\n",
            "Train loss at 200: 13.413431167602539\n",
            "Train loss at 300: 13.065494537353516\n",
            "Train loss at 400: 16.349767684936523\n",
            "Train loss at 500: 9.526177406311035\n",
            "Train loss at 600: 16.262428283691406\n",
            "Train loss at 700: 5.820808410644531\n",
            "Train loss at 800: 14.459640502929688\n",
            "Train loss at 900: 11.719948768615723\n",
            "EVALUATING:\n",
            "Valid_loss 13.56952188396063\n",
            "[[970 102  62 451]\n",
            " [343 768  26  54]\n",
            " [ 89  13 348  46]\n",
            " [ 42  11   6 558]]\n",
            "comment F1-score: 0.6404754044239024\n",
            "unrelated F1-score: 0.7366906474820144\n",
            "support F1-score: 0.7420042643923241\n",
            "refute F1-score: 0.6465816917728853\n",
            "Accu: 0.6798662895345847\n",
            "F1-Weighted 0.6838588573220435\n",
            "F1-Avg 0.6914380020177815\n",
            "[0/5]     train_loss: 12.54105 valid_loss: 13.56952\n",
            "\n",
            "\n",
            "========= Beginning 2 epoch ==========\n",
            "Train loss at 0: 9.691740036010742\n",
            "Train loss at 100: 10.369770050048828\n",
            "Train loss at 200: 4.688626289367676\n",
            "Train loss at 300: 12.526899337768555\n",
            "Train loss at 400: 11.055965423583984\n",
            "Train loss at 500: 13.812780380249023\n",
            "Train loss at 600: 16.634540557861328\n",
            "Train loss at 700: 5.068998336791992\n",
            "Train loss at 800: 10.288418769836426\n",
            "Train loss at 900: 8.515953063964844\n",
            "EVALUATING:\n",
            "Valid_loss 12.133755357050505\n",
            "[[1070  200   88  227]\n",
            " [ 211  941   17   22]\n",
            " [ 106   30  342   18]\n",
            " [  76   31   19  491]]\n",
            "comment F1-score: 0.7020997375328084\n",
            "unrelated F1-score: 0.7864605098203091\n",
            "support F1-score: 0.7110187110187111\n",
            "refute F1-score: 0.7141818181818181\n",
            "Accu: 0.7312933916173824\n",
            "F1-Weighted 0.7309894609588459\n",
            "F1-Avg 0.7284401941384118\n",
            "[1/5]     train_loss: 9.70452 valid_loss: 12.13376\n",
            "\n",
            "\n",
            "========= Beginning 3 epoch ==========\n",
            "Train loss at 0: 12.54499626159668\n",
            "Train loss at 100: 8.530450820922852\n",
            "Train loss at 200: 7.123714447021484\n",
            "Train loss at 300: 11.472386360168457\n",
            "Train loss at 400: 11.35003662109375\n",
            "Train loss at 500: 9.904623031616211\n",
            "Train loss at 600: 7.786433696746826\n",
            "Train loss at 700: 3.9065489768981934\n",
            "Train loss at 800: 9.749307632446289\n",
            "Train loss at 900: 13.734139442443848\n",
            "EVALUATING:\n",
            "Valid_loss 12.90920910483501\n",
            "[[948 161 187 289]\n",
            " [182 919  48  42]\n",
            " [ 53  16 403  24]\n",
            " [ 51  18  32 516]]\n",
            "comment F1-score: 0.6725789286981199\n",
            "unrelated F1-score: 0.7973969631236442\n",
            "support F1-score: 0.6912521440823328\n",
            "refute F1-score: 0.6935483870967741\n",
            "Accu: 0.716379532013371\n",
            "F1-Weighted 0.7165126776472941\n",
            "F1-Avg 0.7136941057502177\n",
            "[2/5]     train_loss: 7.71892 valid_loss: 12.90921\n",
            "\n",
            "\n",
            "========= Beginning 4 epoch ==========\n",
            "Train loss at 0: 7.328449726104736\n",
            "Train loss at 100: 6.692278861999512\n",
            "Train loss at 200: 6.347149848937988\n",
            "Train loss at 300: 9.85848617553711\n",
            "Train loss at 400: 8.431262969970703\n",
            "Train loss at 500: 10.220720291137695\n",
            "Train loss at 600: 10.664999008178711\n",
            "Train loss at 700: 3.126966714859009\n",
            "Train loss at 800: 12.09597110748291\n",
            "Train loss at 900: 14.055770874023438\n",
            "EVALUATING:\n",
            "Valid_loss 13.17617638286997\n",
            "[[1074  153  151  207]\n",
            " [ 209  894   62   26]\n",
            " [  69   13  395   19]\n",
            " [  90   24   48  455]]\n",
            "comment F1-score: 0.709613478691774\n",
            "unrelated F1-score: 0.785934065934066\n",
            "support F1-score: 0.685763888888889\n",
            "refute F1-score: 0.6873111782477341\n",
            "Accu: 0.7246078683466187\n",
            "F1-Weighted 0.7264064597895795\n",
            "F1-Avg 0.7171556529406158\n",
            "[3/5]     train_loss: 6.97168 valid_loss: 13.17618\n",
            "\n",
            "\n",
            "========= Beginning 5 epoch ==========\n",
            "Train loss at 0: 5.9970269203186035\n",
            "Train loss at 100: 8.487934112548828\n",
            "Train loss at 200: 21.226593017578125\n",
            "Train loss at 300: 21.308801651000977\n",
            "Train loss at 400: 19.779083251953125\n",
            "Train loss at 500: 19.726530075073242\n",
            "Train loss at 600: 20.672645568847656\n",
            "Train loss at 700: 18.22630500793457\n",
            "Train loss at 800: 20.784196853637695\n",
            "Train loss at 900: 21.712020874023438\n",
            "EVALUATING:\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Valid_loss 22.1631711960816\n",
            "[[1585    0    0    0]\n",
            " [1191    0    0    0]\n",
            " [ 496    0    0    0]\n",
            " [ 617    0    0    0]]\n",
            "comment F1-score: 0.5791012056996712\n",
            "unrelated F1-score: 0.0\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.40755978400617127\n",
            "F1-Weighted 0.23601836231267134\n",
            "F1-Avg 0.1447753014249178\n",
            "[4/5]     train_loss: 17.56478 valid_loss: 22.16317\n",
            "Total time taken (mins):  33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "!python gdrive/My\\ Drive/NLP\\ Stance\\ Detection/Code_Kaushal/train.py --dataset_name=wtwt --target_merger='AET_HUM' --test_mode=False --bert_type=vinai/bertweet-base\n",
        "print('Total time taken (mins): ', int((time.time()-start_time)/60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjnHOPxe0ldd",
        "outputId": "33337080-1b10-4c9a-d534-9ab8c45d6e07"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++++++++++++++++++++++\n",
            "Target = True\n",
            "++++++++++++++++++++++\n",
            "{'comment': 0, 'unrelated': 1, 'support': 2, 'refute': 3} || {0: 'comment', 1: 'unrelated', 2: 'support', 3: 'refute'}\n",
            "Length of train, valid, test: 22360 0 6996\n",
            "Length of train, eval_set: 17888 4472\n",
            "Before shuffling train[0] =  971761970117357568 | After shuffling train[0] =  940983804319191042\n",
            "/content/gdrive/My Drive/NLP Stance Detection/Code_Kaushal/saves/wtwt_AET_HUM_target\n",
            "1. Type of train dataset\n",
            "Length of train: 17888\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
            "Loaded Bert Tokenizer\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Train_dataset: num_batches= 1118  | num_data= 17888\n",
            "97\n",
            "Eval_dataset: num_batches= 280  | num_data= 4472\n",
            "97\n",
            "Dataset created\n",
            "Sun Apr 10 05:33:07 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    56W / 149W |    339MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Model created\n",
            "Sun Apr 10 05:33:10 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    56W / 149W |    339MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Embedding Shape: torch.Size([64004, 768])\n",
            "134905348\n",
            "Detected 1 GPUs!\n",
            "\n",
            "\n",
            "========= Beginning 1 epoch ==========\n",
            "Train loss at 0: 22.325366973876953\n",
            "Train loss at 100: 14.180878639221191\n",
            "Train loss at 200: 19.289226531982422\n",
            "Train loss at 300: 18.408082962036133\n",
            "Train loss at 400: 20.85395622253418\n",
            "Train loss at 500: 24.702255249023438\n",
            "Train loss at 600: 17.400440216064453\n",
            "Train loss at 700: 6.143463611602783\n",
            "Train loss at 800: 14.289137840270996\n",
            "Train loss at 900: 16.016910552978516\n",
            "Train loss at 1000: 8.91267204284668\n",
            "Train loss at 1100: 11.1865234375\n",
            "EVALUATING:\n",
            "Valid_loss 11.122523636477334\n",
            "[[1112  213  102   86]\n",
            " [ 155 1459   16   16]\n",
            " [  92   45  245    4]\n",
            " [ 325   37   30  535]]\n",
            "comment F1-score: 0.6956521739130435\n",
            "unrelated F1-score: 0.8582352941176471\n",
            "support F1-score: 0.6290115532734275\n",
            "refute F1-score: 0.6823979591836734\n",
            "Accu: 0.7493291592128801\n",
            "F1-Weighted 0.7469942757099486\n",
            "F1-Avg 0.7163242451219479\n",
            "[0/5]     train_loss: 15.05197 valid_loss: 11.12252\n",
            "\n",
            "\n",
            "========= Beginning 2 epoch ==========\n",
            "Train loss at 0: 12.109315872192383\n",
            "Train loss at 100: 6.170769214630127\n",
            "Train loss at 200: 15.684925079345703\n",
            "Train loss at 300: 18.907989501953125\n",
            "Train loss at 400: 21.00998878479004\n",
            "Train loss at 500: 21.811756134033203\n",
            "Train loss at 600: 22.58193588256836\n",
            "Train loss at 700: 17.953338623046875\n",
            "Train loss at 800: 18.10666275024414\n",
            "Train loss at 900: 21.324054718017578\n",
            "Train loss at 1000: 21.02030372619629\n",
            "Train loss at 1100: 19.79399299621582\n",
            "EVALUATING:\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Valid_loss 21.349605168615067\n",
            "[[1513    0    0    0]\n",
            " [1646    0    0    0]\n",
            " [ 386    0    0    0]\n",
            " [ 927    0    0    0]]\n",
            "comment F1-score: 0.5055973266499582\n",
            "unrelated F1-score: 0.0\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.33832737030411447\n",
            "F1-Weighted 0.17105741395827073\n",
            "F1-Avg 0.12639933166248954\n",
            "[1/5]     train_loss: 19.02120 valid_loss: 21.34961\n",
            "\n",
            "\n",
            "========= Beginning 3 epoch ==========\n",
            "Train loss at 0: 20.299034118652344\n",
            "Train loss at 100: 16.00411605834961\n",
            "Train loss at 200: 22.915267944335938\n",
            "Train loss at 300: 18.677906036376953\n",
            "Train loss at 400: 21.34429168701172\n",
            "Train loss at 500: 22.144075393676758\n",
            "Train loss at 600: 22.627267837524414\n",
            "Train loss at 700: 17.99994659423828\n",
            "Train loss at 800: 18.138399124145508\n",
            "Train loss at 900: 21.669570922851562\n",
            "Train loss at 1000: 21.439624786376953\n",
            "Train loss at 1100: 19.94638442993164\n",
            "EVALUATING:\n",
            "Valid_loss 21.490901844842092\n",
            "[[1513    0    0    0]\n",
            " [1646    0    0    0]\n",
            " [ 386    0    0    0]\n",
            " [ 927    0    0    0]]\n",
            "comment F1-score: 0.5055973266499582\n",
            "unrelated F1-score: 0.0\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.33832737030411447\n",
            "F1-Weighted 0.17105741395827073\n",
            "F1-Avg 0.12639933166248954\n",
            "[2/5]     train_loss: 20.23755 valid_loss: 21.49090\n",
            "\n",
            "\n",
            "========= Beginning 4 epoch ==========\n",
            "Train loss at 0: 20.065567016601562\n",
            "Train loss at 100: 16.35970687866211\n",
            "Train loss at 200: 23.279075622558594\n",
            "Train loss at 300: 18.75691795349121\n",
            "Train loss at 400: 21.18793487548828\n",
            "Train loss at 500: 21.90515899658203\n",
            "Train loss at 600: 22.796491622924805\n",
            "Train loss at 700: 17.692062377929688\n",
            "Train loss at 800: 18.125457763671875\n",
            "Train loss at 900: 21.61758041381836\n",
            "Train loss at 1000: 21.508535385131836\n",
            "Train loss at 1100: 20.064529418945312\n",
            "EVALUATING:\n",
            "Valid_loss 21.657984597342356\n",
            "[[1513    0    0    0]\n",
            " [1646    0    0    0]\n",
            " [ 386    0    0    0]\n",
            " [ 927    0    0    0]]\n",
            "comment F1-score: 0.5055973266499582\n",
            "unrelated F1-score: 0.0\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.33832737030411447\n",
            "F1-Weighted 0.17105741395827073\n",
            "F1-Avg 0.12639933166248954\n",
            "[3/5]     train_loss: 20.22156 valid_loss: 21.65798\n",
            "\n",
            "\n",
            "========= Beginning 5 epoch ==========\n",
            "Train loss at 0: 20.230520248413086\n",
            "Train loss at 100: 16.416933059692383\n",
            "Train loss at 200: 22.919023513793945\n",
            "Train loss at 300: 18.84374237060547\n",
            "Train loss at 400: 21.35753631591797\n",
            "Train loss at 500: 22.042604446411133\n",
            "Train loss at 600: 22.993057250976562\n",
            "Train loss at 700: 17.645103454589844\n",
            "Train loss at 800: 18.14698028564453\n",
            "Train loss at 900: 21.49733543395996\n",
            "Train loss at 1000: 21.29243278503418\n",
            "Train loss at 1100: 19.794235229492188\n",
            "EVALUATING:\n",
            "Valid_loss 21.749484607151576\n",
            "[[1513    0    0    0]\n",
            " [1646    0    0    0]\n",
            " [ 386    0    0    0]\n",
            " [ 927    0    0    0]]\n",
            "comment F1-score: 0.5055973266499582\n",
            "unrelated F1-score: 0.0\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.33832737030411447\n",
            "F1-Weighted 0.17105741395827073\n",
            "F1-Avg 0.12639933166248954\n",
            "[4/5]     train_loss: 20.20145 valid_loss: 21.74948\n",
            "Total time taken (mins):  37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "!python gdrive/My\\ Drive/NLP\\ Stance\\ Detection/Code_Kaushal/train.py --dataset_name=wtwt --target_merger='CI_ESRX' --test_mode=True --bert_type=vinai/bertweet-base\n",
        "print('Total time taken (mins): ', int((time.time()-start_time)/60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coJJ1POg9faC",
        "outputId": "71f88c18-5873-4b8c-8056-c26006d8832d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++++++++++++++++++++++\n",
            "Target = True\n",
            "++++++++++++++++++++++\n",
            "{'comment': 0, 'unrelated': 1, 'support': 2, 'refute': 3} || {0: 'comment', 1: 'unrelated', 2: 'support', 3: 'refute'}\n",
            "Length of train, valid, test: 27130 0 2226\n",
            "Length of train, eval_set: 27130 2226\n",
            "Before shuffling train[0] =  944165633889095681 | After shuffling train[0] =  831878268378816513\n",
            "/content/gdrive/My Drive/NLP Stance Detection/Code_Kaushal/saves/wtwt_CI_ESRX_target\n",
            "1. Type of train dataset\n",
            "Length of train: 27130\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
            "Loaded Bert Tokenizer\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Train_dataset: num_batches= 1696  | num_data= 27130\n",
            "97\n",
            "Eval_dataset: num_batches= 140  | num_data= 2226\n",
            "97\n",
            "Dataset created\n",
            "Sun Apr 10 06:17:51 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    57W / 149W |    347MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Model created\n",
            "Sun Apr 10 06:17:54 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    57W / 149W |    347MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Embedding Shape: torch.Size([64004, 768])\n",
            "134905348\n",
            "Detected 1 GPUs!\n",
            "\n",
            "\n",
            "========= Beginning 1 epoch ==========\n",
            "Train loss at 0: 22.219301223754883\n",
            "Train loss at 100: 10.87669849395752\n",
            "Train loss at 200: 11.60993480682373\n",
            "Train loss at 300: 6.6688408851623535\n",
            "Train loss at 400: 9.261723518371582\n",
            "Train loss at 500: 13.811444282531738\n",
            "Train loss at 600: 4.690306663513184\n",
            "Train loss at 700: 8.1101655960083\n",
            "Train loss at 800: 7.552820205688477\n",
            "Train loss at 900: 9.698983192443848\n",
            "Train loss at 1000: 15.572582244873047\n",
            "Train loss at 1100: 4.982302665710449\n",
            "Train loss at 1200: 7.189706325531006\n",
            "Train loss at 1300: 8.597211837768555\n",
            "Train loss at 1400: 9.48330307006836\n",
            "Train loss at 1500: 12.561429023742676\n",
            "Train loss at 1600: 11.438940048217773\n",
            "EVALUATING:\n",
            "Valid_loss 15.170145138672419\n",
            "[[557 196  76  23]\n",
            " [ 58 342  31  30]\n",
            " [131  76 459  22]\n",
            " [104  18   7  96]]\n",
            "comment F1-score: 0.654524089306698\n",
            "unrelated F1-score: 0.62580054894785\n",
            "support F1-score: 0.7279936558287075\n",
            "refute F1-score: 0.48484848484848486\n",
            "Accu: 0.6531895777178796\n",
            "F1-Weighted 0.6541325792701371\n",
            "F1-Avg 0.6232916947329351\n",
            "[0/5]     train_loss: 10.53912 valid_loss: 15.17015\n",
            "\n",
            "\n",
            "========= Beginning 2 epoch ==========\n",
            "Train loss at 0: 6.60715389251709\n",
            "Train loss at 100: 11.857746124267578\n",
            "Train loss at 200: 7.668467044830322\n",
            "Train loss at 300: 6.954254150390625\n",
            "Train loss at 400: 7.7393598556518555\n",
            "Train loss at 500: 11.906694412231445\n",
            "Train loss at 600: 2.3042116165161133\n",
            "Train loss at 800: 7.630862236022949\n",
            "Train loss at 900: 6.4146647453308105\n",
            "Train loss at 1000: 14.727062225341797\n",
            "Train loss at 1100: 4.864998817443848\n",
            "Train loss at 1200: 7.378816604614258\n",
            "Train loss at 1300: 4.5424604415893555\n",
            "Train loss at 1400: 8.478086471557617\n",
            "Train loss at 1500: 22.770631790161133\n",
            "Train loss at 1600: 21.429107666015625\n",
            "EVALUATING:\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Valid_loss 21.282231981413705\n",
            "[[  0 852   0   0]\n",
            " [  0 461   0   0]\n",
            " [  0 688   0   0]\n",
            " [  0 225   0   0]]\n",
            "comment F1-score: 0.0\n",
            "unrelated F1-score: 0.34313360625232603\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.20709793351302785\n",
            "F1-Weighted 0.0710622607737297\n",
            "F1-Avg 0.08578340156308151\n",
            "[1/5]     train_loss: 9.78427 valid_loss: 21.28223\n",
            "\n",
            "\n",
            "========= Beginning 3 epoch ==========\n",
            "Train loss at 0: 22.277503967285156\n",
            "Train loss at 100: 20.360586166381836\n",
            "Train loss at 200: 19.356407165527344\n",
            "Train loss at 300: 19.655851364135742\n",
            "Train loss at 400: 20.368183135986328\n",
            "Train loss at 500: 21.34236717224121\n",
            "Train loss at 600: 18.72800636291504\n",
            "Train loss at 700: 16.439138412475586\n",
            "Train loss at 800: 24.056461334228516\n",
            "Train loss at 900: 18.8583927154541\n",
            "Train loss at 1000: 22.27682113647461\n",
            "Train loss at 1100: 18.787982940673828\n",
            "Train loss at 1200: 20.46442413330078\n",
            "Train loss at 1300: 20.29852867126465\n",
            "Train loss at 1400: 21.75789451599121\n",
            "Train loss at 1500: 20.149263381958008\n",
            "Train loss at 1600: 21.20758819580078\n",
            "EVALUATING:\n",
            "Valid_loss 21.12981834071023\n",
            "[[852   0   0   0]\n",
            " [461   0   0   0]\n",
            " [688   0   0   0]\n",
            " [225   0   0   0]]\n",
            "comment F1-score: 0.5536062378167641\n",
            "unrelated F1-score: 0.0\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.38274932614555257\n",
            "F1-Weighted 0.21189241447434098\n",
            "F1-Avg 0.13840155945419103\n",
            "[2/5]     train_loss: 20.49398 valid_loss: 21.12982\n",
            "\n",
            "\n",
            "========= Beginning 4 epoch ==========\n",
            "Train loss at 0: 21.997325897216797\n",
            "Train loss at 100: 21.006603240966797\n",
            "Train loss at 200: 18.725831985473633\n",
            "Train loss at 300: 18.18463897705078\n",
            "Train loss at 400: 20.23356819152832\n",
            "Train loss at 500: 21.06370735168457\n",
            "Train loss at 600: 18.410757064819336\n",
            "Train loss at 700: 16.832382202148438\n",
            "Train loss at 800: 24.394758224487305\n",
            "Train loss at 900: 18.849924087524414\n",
            "Train loss at 1000: 22.860031127929688\n",
            "Train loss at 1100: 18.626768112182617\n",
            "Train loss at 1200: 20.219083786010742\n",
            "Train loss at 1300: 20.1392822265625\n",
            "Train loss at 1400: 21.698965072631836\n",
            "Train loss at 1500: 20.16417694091797\n",
            "Train loss at 1600: 21.337841033935547\n",
            "EVALUATING:\n",
            "Valid_loss 21.464442685672214\n",
            "[[852   0   0   0]\n",
            " [461   0   0   0]\n",
            " [688   0   0   0]\n",
            " [225   0   0   0]]\n",
            "comment F1-score: 0.5536062378167641\n",
            "unrelated F1-score: 0.0\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.38274932614555257\n",
            "F1-Weighted 0.21189241447434098\n",
            "F1-Avg 0.13840155945419103\n",
            "[3/5]     train_loss: 20.42704 valid_loss: 21.46444\n",
            "\n",
            "\n",
            "========= Beginning 5 epoch ==========\n",
            "Train loss at 0: 21.902284622192383\n",
            "Train loss at 100: 20.97664451599121\n",
            "Train loss at 200: 18.978919982910156\n",
            "Train loss at 300: 18.251052856445312\n",
            "Train loss at 400: 20.323036193847656\n",
            "Train loss at 500: 21.095996856689453\n",
            "Train loss at 600: 18.192642211914062\n",
            "Train loss at 700: 16.837453842163086\n",
            "Train loss at 800: 24.188457489013672\n",
            "Train loss at 900: 18.907283782958984\n",
            "Train loss at 1000: 22.753665924072266\n",
            "Train loss at 1100: 18.578643798828125\n",
            "Train loss at 1200: 20.444501876831055\n",
            "Train loss at 1300: 20.29966926574707\n",
            "Train loss at 1400: 21.571943283081055\n",
            "Train loss at 1500: 20.230297088623047\n",
            "Train loss at 1600: 21.265506744384766\n",
            "EVALUATING:\n",
            "Valid_loss 21.65709913798741\n",
            "[[852   0   0   0]\n",
            " [461   0   0   0]\n",
            " [688   0   0   0]\n",
            " [225   0   0   0]]\n",
            "comment F1-score: 0.5536062378167641\n",
            "unrelated F1-score: 0.0\n",
            "support F1-score: 0.0\n",
            "refute F1-score: 0.0\n",
            "Accu: 0.38274932614555257\n",
            "F1-Weighted 0.21189241447434098\n",
            "F1-Avg 0.13840155945419103\n",
            "[4/5]     train_loss: 20.40804 valid_loss: 21.65710\n",
            "/content/gdrive/My Drive/NLP Stance Detection/Code_Kaushal/saves/wtwt_CI_ESRX_target\n",
            "Total time taken (mins):  50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SRxL7JWgHuWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}